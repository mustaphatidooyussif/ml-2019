{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LibiumNet.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "-NKJhfg4ZpkT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#LibiumNet: Lip-Reading using RCNN"
      ]
    },
    {
      "metadata": {
        "id": "tkH0NM_xmmE7",
        "colab_type": "code",
        "outputId": "fff4b4fb-e658-40e9-e817-e14ae9a6ebbb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "import os, glob\n",
        "import imageio\n",
        "import itertools\n",
        "import math\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras.layers import Dense, Activation, Dropout, Flatten, TimeDistributed, LSTM, Input, CuDNNLSTM, BatchNormalization, Conv2D, MaxPooling2D, Reshape, Conv1D, GlobalAveragePooling1D, MaxPooling1D, Lambda\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "imageio.plugins.ffmpeg.download()\n",
        "tf.enable_eager_execution()\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Imageio: 'ffmpeg-linux64-v3.3.1' was not found on your computer; downloading it now.\n",
            "Try 1. Download from https://github.com/imageio/imageio-binaries/raw/master/ffmpeg/ffmpeg-linux64-v3.3.1 (43.8 MB)\n",
            "Downloading: 8192/45929032 bytes (0.0%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b4046848/45929032 bytes (8.8%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b8224768/45929032 bytes (17.9%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12402688/45929032 bytes (27.0%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16531456/45929032 bytes (36.0%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b20684800/45929032 bytes (45.0%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b24764416/45929032 bytes (53.9%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b28901376/45929032 bytes (62.9%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b33095680/45929032 bytes (72.1%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b37175296/45929032 bytes (80.9%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b41336832/45929032 bytes (90.0%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b45441024/45929032 bytes (98.9%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b45929032/45929032 bytes (100.0%)\n",
            "  Done\n",
            "File saved as /root/.imageio/ffmpeg/ffmpeg-linux64-v3.3.1.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "_z6oXdzSrj4A",
        "colab_type": "code",
        "outputId": "4de8c327-985d-48b2-e586-ebcbc7e35af1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "cell_type": "code",
      "source": [
        "tf.VERSION"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1.13.1'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "metadata": {
        "id": "K13X7Hdg3Ktp",
        "colab_type": "code",
        "outputId": "c5403c56-c284-4e6d-bb55-89785137ae05",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "cell_type": "code",
      "source": [
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "R7_73zWRZHzw",
        "colab_type": "code",
        "outputId": "e4e05320-401b-47ad-f1aa-396d8437414a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 131
        }
      },
      "cell_type": "code",
      "source": [
        "# mounting notebook to google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/gdrive/')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /gdrive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "D-fZdjfygGXe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "\"\"\"\n",
        "This model generates generator of the datasets for the Network. \n",
        "\n",
        "@authors : Mustapha Tidoo Yussif, Samuel Atule, Jean Sabastien Dovonon\n",
        "         and Nutifafa Amedior. \n",
        "\"\"\"\n",
        "IMAGE_HEIGHT = 256\n",
        "IMAGE_WIDTH = 256\n",
        "IMAGE_CHANNEL = 3\n",
        "NUM_FRAMES = 29\n",
        "NUM_CLASSES = 2\n",
        "        \n",
        "        \n",
        "class GenerateDataset(object):\n",
        "    \"\"\"Generates generator for the datasets\n",
        "    \n",
        "    This model generates a generator for the datasets. This done to efficiently \n",
        "    manage space.\n",
        "    \n",
        "    :param: file_path: path to files/videos.\n",
        "    :param directory: Path to the main directory.\n",
        "    \"\"\"\n",
        "    def __init__(self, file_path, directory, n_items):\n",
        "        self.n_items = n_items\n",
        "        self.directory = directory\n",
        "        self.file_path = file_path\n",
        "        self.num_samples = len(self.samples(self.get_video_files(self.file_path, self.directory)))\n",
        "        \n",
        "\n",
        "    def load_video(self, filename):\n",
        "        \"\"\"Loads the specified video using ffmpeg.\n",
        "\n",
        "        Returns:\n",
        "            List[FloatTensor]: the frames of the video as a list of 3D tensors\n",
        "                (channels, width, height)\"\"\"\n",
        "        \n",
        "        reader = imageio.get_reader(filename,  'ffmpeg')\n",
        "        \n",
        "        return np.array(list(reader), dtype=np.float32)\n",
        "    \n",
        "    def resize_frames(self, frames):\n",
        "        \"\"\"\n",
        "        Crops the frames of the videos around the mouth region.\n",
        "        This is the part that is most important part and relevant\n",
        "        to the model (where we can get the relevant features)\n",
        "\n",
        "        :param frames: The frames in the video. \n",
        "        :return: returns the croped frames.\n",
        "        \"\"\"\n",
        "        tf.image.resize_images(X, (IMAGE_SIZE, IMAGE_SIZE), \n",
        "                                    tf.image.ResizeMethod.NEAREST_NEIGHBOR) \n",
        "\n",
        "    def get_sample_size(self):\n",
        "      return self.num_samples\n",
        "    \n",
        "    \n",
        "    def create_df(self, file_path):\n",
        "        '''\n",
        "        creates pandas dataframe of labels and words directories\n",
        "        '''\n",
        "        \n",
        "        d = {}\n",
        "        y_labels = []\n",
        "        class_folders = []\n",
        "        for ind, clss in enumerate(os.listdir(file_path)):\n",
        "            y_labels.append(ind)\n",
        "            class_folders.append(clss)\n",
        "        \n",
        "        d['directory'] = class_folders\n",
        "        d['class'] = y_labels\n",
        "        print(d)\n",
        "        return pd.DataFrame(d)\n",
        "\n",
        "\n",
        "    def get_video_files(self, file_path, directory=None):\n",
        "        '''\n",
        "        get video files from word class directories\n",
        "        '''\n",
        "        d = {}\n",
        "        f = []\n",
        "        \n",
        "        for root, dirs, files in os.walk(file_path):\n",
        "            if root.split('/')[-1] == directory:\n",
        "                for file in files:\n",
        "                    if file.endswith(\".mp4\"):\n",
        "                        target_file = file.split('_')[0]\n",
        "                        f.append(target_file)\n",
        "                        if target_file not in d:\n",
        "                            d[target_file] = []\n",
        "                        d[target_file].append(os.path.join(root, file))\n",
        "                    \n",
        "        return d\n",
        "        \n",
        "    def generator(self, batch = 1):\n",
        "        \"\"\"Interfaces the private generator method\n",
        "\n",
        "        :param num_items_per_class: The number of items in a categority. \n",
        "        :param batch: The batch size.\n",
        "        \"\"\"\n",
        "        data = self.create_df(self.file_path)\n",
        "        video_files = self.get_video_files(self.file_path, self.directory)\n",
        "        return self._generator(data, directory = self.directory, video_files = video_files, BATCH_SIZE = batch)\n",
        "\n",
        "    def samples(self, video_files):\n",
        "      train = []\n",
        "      for key, value in video_files.items():\n",
        "        ind = 0\n",
        "        for file in value:\n",
        "          train.append(file)\n",
        "          ind+=1\n",
        "          if ind == self.n_items:\n",
        "            break\n",
        "          \n",
        "      return train\n",
        "    \n",
        "    def _generator(self, data, directory=None, video_files=None, BATCH_SIZE = 64):\n",
        "        \n",
        "        '''\n",
        "        retrieves the training batch for each iteration\n",
        "        '''\n",
        "        \n",
        "        train = []\n",
        "        for key, value in video_files.items():\n",
        "            ind = 0\n",
        "            for file in value:\n",
        "                train.append(file)\n",
        "                ind+=1\n",
        "                if ind == self.n_items:\n",
        "                  break\n",
        "                \n",
        "                  \n",
        "                \n",
        "        while True:\n",
        "            # Randomize the indices to make an array\n",
        "            indices_arr = np.random.permutation(len(train))\n",
        "            \n",
        "            for batch in range(0, len(indices_arr), BATCH_SIZE):\n",
        "                # slice out the current batch according to batch-size\n",
        "                current_batch = indices_arr[batch:(batch + BATCH_SIZE)]\n",
        "\n",
        "                # initializing the arrays, x_train and y_train\n",
        "                x_train = np.empty([0, NUM_FRAMES, IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_CHANNEL], dtype=np.float32)\n",
        "            \n",
        "                y_train = np.empty([0], dtype=np.int32)\n",
        "\n",
        "                for i in current_batch:\n",
        "                    # get an image and its corresponding color for an traffic light\n",
        "                    video_frames = self.load_video(train[i])\n",
        "                    \n",
        "                    \n",
        "                    #preprocess frames from videos\n",
        "#                     video_frames = tf.image.resize_nearest_neighbor(video_frames,(IMAGE_HEIGHT, IMAGE_WIDTH), )\n",
        "                    #video_frames = tf.image.rgb_to_grayscale(video_frames)\n",
        "#                     video_frames = tf.reshape(video_frames, (NUM_FRAMES, IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_CHANNEL))\n",
        "\n",
        "                    # Appending them to existing batch\n",
        "                    x_train = np.append(x_train, [video_frames/255], axis=0)\n",
        "                    y_train = np.append(y_train, [ data.loc[ data['directory'] == train[i].split('/')[-1].split('_')[-2] ].values[0][1] ])\n",
        "                    #print(data.loc[ data['directory'] == train[i].split('/')[-1].split('_')[-2] ].values[0][1])\n",
        "                    \n",
        "                \n",
        "                y_train = to_categorical(y_train, num_classes=NUM_CLASSES)\n",
        "\n",
        "                yield(x_train, y_train)\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "46N4_yDpA1Mc",
        "colab_type": "code",
        "outputId": "4a413eae-e831-4196-9763-756269a780bc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "cell_type": "code",
      "source": [
        "gen = GenerateDataset('/gdrive/My Drive/LibiumNet/Lipread_mp4/', 'train', 2)\n",
        "datasets = gen.generator()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'directory': ['HOSPITAL', 'LEADER'], 'class': [0, 1]}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "IeV-IN7ZY6U6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class LibiumNet(object):\n",
        "    \"\"\"TA lipreading model, `LibiunNet`\n",
        "    This is lip reading model which reads or predicts the words of a spoken mouth in a silent video. \n",
        "    This model implements the RCNN (Recurrent Convolutional Neural Network) architecture. \n",
        "\n",
        "    :param img_c: The number of channels of the input image. i.e. a frame in a video (default 3).\n",
        "    :param img_w: The width of the input image i.e. a frame in a video (default 256)\n",
        "    :param img_h: The height of the input image i.e. a frame in a video (default 256)\n",
        "    :param frames_n: The total number of frames in an input video (default 29)\n",
        "    :param output_size: The output size of the network. \n",
        "    \n",
        "    \"\"\"\n",
        "    def __init__(self, img_c=3, img_w=256, img_h=256, frames_n=29, output_size=10):\n",
        "        self.img_c = img_c\n",
        "        self.img_w = img_w\n",
        "        self.img_h = img_h\n",
        "        self.frames_n = frames_n\n",
        "        self.output_size = output_size\n",
        "        self.build()\n",
        "    \n",
        "    def build(self):\n",
        "        \"\"\"\n",
        "        Retrieves the features from the last pool layer in the densenet pretrained model \n",
        "        and pass obtained features to LSTM network. \n",
        "        \"\"\"\n",
        "        input_shape = (self.frames_n, self.img_w, self.img_h, self.img_c) # input shape\n",
        "\n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        feature_extractor = Sequential()\n",
        "        inputShape = (self.img_w, self.img_h, self.img_c)\n",
        "        chanDim = -1\n",
        "        \n",
        "        feature_extractor.add(Lambda(lambda x: tf.keras.backend.mean(x, axis=3, keepdims=True), input_shape=inputShape))\n",
        "        feature_extractor.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "        \n",
        "        # first CONV => RELU => CONV => RELU => POOL layer set\n",
        "        \n",
        "        feature_extractor.add(Conv2D(32, (3, 3)))\n",
        "        feature_extractor.add(Activation(\"relu\"))\n",
        "        feature_extractor.add(BatchNormalization(axis=chanDim))\n",
        "        feature_extractor.add(Conv2D(32, (3, 3)))\n",
        "        feature_extractor.add(Activation(\"relu\"))\n",
        "        feature_extractor.add(BatchNormalization(axis=chanDim))\n",
        "        feature_extractor.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "        feature_extractor.add(Dropout(0.25))\n",
        "        \n",
        "        # second CONV => RELU => CONV => RELU => POOL layer set\n",
        "        feature_extractor.add(Conv2D(64, (3, 3)))\n",
        "        feature_extractor.add(Activation(\"relu\"))\n",
        "        feature_extractor.add(BatchNormalization(axis=chanDim))\n",
        "        feature_extractor.add(Conv2D(64, (3, 3)))\n",
        "        feature_extractor.add(Activation(\"relu\"))\n",
        "        feature_extractor.add(BatchNormalization(axis=chanDim))\n",
        "        feature_extractor.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "        feature_extractor.add(Dropout(0.25))\n",
        "        \n",
        "        \n",
        "        TIME_PERIODS = self.frames_n\n",
        "        dims = 53824\n",
        "\n",
        "        model_m = Sequential()\n",
        "        model_m.add(Conv1D(10, 2, activation='relu', input_shape=(TIME_PERIODS, dims)))\n",
        "        model_m.add(Conv1D(10, 2, activation='relu'))\n",
        "        #model_m.add(MaxPooling1D(3))\n",
        "        #model_m.add(Conv1D(10, 3, activation='relu'))\n",
        "        #model_m.add(Conv1D(10, 3, activation='relu'))\n",
        "        #model_m.add(GlobalAveragePooling1D())\n",
        "        #model_m.add(Dropout(0.5))\n",
        "        \n",
        "        \n",
        "        self.input_data = Input(name='the_input', shape=input_shape, dtype='float32')\n",
        "        self.image_frame_features = TimeDistributed(feature_extractor)(self.input_data) ## extracting the features from the images\n",
        "        \n",
        "        self.flat = TimeDistributed(Flatten())(self.image_frame_features) ## flatten before passing on to the recurrent network\n",
        "\n",
        "        self.sequence1 = model_m(self.flat)\n",
        "        self.sequence = CuDNNLSTM(16)(self.sequence1) \n",
        "        \n",
        "        self.dense3 = Dense(10, activation='relu')(self.sequence)\n",
        "        self.dense2 = Dense(10, activation='relu')(self.dense3)\n",
        "        self.dense = Dense(self.output_size, name='logits')(self.dense2)\n",
        "\n",
        "        self.pred = Activation('softmax', name='softmax')(self.dense)\n",
        "\n",
        "\n",
        "        self.model = Model(inputs = self.input_data, outputs=self.pred)\n",
        "\n",
        "\n",
        "    def summary(self):\n",
        "        \"\"\"\"Summarizes the architecture of the model.\n",
        "        \n",
        "        :return: returns the model architecture summary\n",
        "        \"\"\"\n",
        "        return self.model.summary()\n",
        "      \n",
        "    \n",
        "    def train(self, generator,steps_per_epoch=None, epochs=1,validation_data=None, validation_steps=None, filepath=\"/gdrive/My Drive/LibiumNet/checkpoint.h5\"):\n",
        "        # Callbacks\n",
        "        early_stopping_monitor = EarlyStopping(patience=3)\n",
        "        checkpoint = ModelCheckpoint(\n",
        "                            filepath, monitor='val_acc', verbose=1, \n",
        "                            save_best_only=True, mode='max'\n",
        "                    )\n",
        "        \n",
        "        reduce_lr = ReduceLROnPlateau(\n",
        "                            monitor='val_loss', factor=0.2,\n",
        "                              patience=7, min_lr=1e-9\n",
        "                    )\n",
        "        \n",
        "        callbacks_list = [checkpoint, reduce_lr, early_stopping_monitor]\n",
        "\n",
        "        \n",
        "        print('Training...')\n",
        "        \n",
        "        self.model.compile(\n",
        "              optimizer=tf.keras.optimizers.SGD(lr=1e-4, momentum=0.9),\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy']\n",
        "        )\n",
        "        \n",
        "        history = self.model.fit_generator(generator, steps_per_epoch=steps_per_epoch, epochs=epochs, validation_data=validation_data, validation_steps = validation_steps)\n",
        "        \n",
        "        #self.visualize_accuracy(history)\n",
        "        self.visualize_loss(history)\n",
        "      \n",
        "      \n",
        "    def predict(self, input_batch):\n",
        "        \"\"\"Predicts a video\n",
        "        \n",
        "        :param input_batch: A batch of a sequence of frames. \n",
        "        :return: returns the predicted probailities\n",
        "        \"\"\"\n",
        "        return self.model(input_batch)\n",
        "      \n",
        "    def visualize_accuracy(self, history):\n",
        "      \"\"\"Visualize model accuracy\n",
        "      \"\"\"\n",
        "      plt.plot(history.history['acc'], label='training accuracy')\n",
        "      plt.plot(history.history['val_acc'], label='testing accuracy')\n",
        "      plt.title('Accuracy')\n",
        "      plt.xlabel('epochs')\n",
        "      plt.ylabel('accuracy')\n",
        "      plt.legend()\n",
        "      \n",
        "    def visualize_loss(self, history):\n",
        "      \"\"\"Visualizes model loss\"\"\"\n",
        "      plt.plot(history.history['loss'], label='training loss')\n",
        "      plt.plot(history.history['val_loss'], label='testing loss')\n",
        "      plt.title('Loss')\n",
        "      plt.xlabel('epochs')\n",
        "      plt.ylabel('loss')\n",
        "      plt.legend()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KorXcvrLh229",
        "colab_type": "code",
        "outputId": "e164814e-a81c-4ae4-d786-008806ceceab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 485
        }
      },
      "cell_type": "code",
      "source": [
        "model = LibiumNet(output_size=2)\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "the_input (InputLayer)       (None, 29, 256, 256, 3)   0         \n",
            "_________________________________________________________________\n",
            "time_distributed_6 (TimeDist (None, 29, 29, 29, 64)    65760     \n",
            "_________________________________________________________________\n",
            "time_distributed_7 (TimeDist (None, 29, 53824)         0         \n",
            "_________________________________________________________________\n",
            "sequential_7 (Sequential)    (None, 27, 10)            1076700   \n",
            "_________________________________________________________________\n",
            "cu_dnnlstm_3 (CuDNNLSTM)     (None, 16)                1792      \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 10)                170       \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 10)                110       \n",
            "_________________________________________________________________\n",
            "logits (Dense)               (None, 2)                 22        \n",
            "_________________________________________________________________\n",
            "softmax (Activation)         (None, 2)                 0         \n",
            "=================================================================\n",
            "Total params: 1,144,554\n",
            "Trainable params: 1,144,170\n",
            "Non-trainable params: 384\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "gE_yz0glgYMS",
        "colab_type": "code",
        "outputId": "679b94eb-9e2c-439e-a37f-afb41c1d879e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        }
      },
      "cell_type": "code",
      "source": [
        "gen = GenerateDataset('/gdrive/My Drive/LibiumNet/Lipread_mp4/', 'train',100)\n",
        "#gen = GenerateDataset('/gdrive/My Drive/LibiumNet/overfit_test/', 'train')\n",
        "datasets = gen.generator()\n",
        "num_samples = gen.get_sample_size()\n",
        "steps_per_epoch = math.ceil(num_samples / 2)\n",
        "\n",
        "# validation\n",
        "val_gen = GenerateDataset('/gdrive/My Drive/LibiumNet/Lipread_mp4/', 'val',20)\n",
        "#val_gen = GenerateDataset('/gdrive/My Drive/LibiumNet/overfit_test/', 'train')\n",
        "val_datasets = val_gen.generator()\n",
        "num_valid_samples = val_gen.get_sample_size()\n",
        "steps_per_valid_epoch = math.ceil(num_valid_samples / 2)\n",
        "\n",
        "num_samples"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'directory': ['HOSPITAL', 'LEADER'], 'class': [0, 1]}\n",
            "{'directory': ['HOSPITAL', 'LEADER'], 'class': [0, 1]}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "200"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "metadata": {
        "id": "u-jhq3Q6d6TZ",
        "colab_type": "code",
        "outputId": "7b6713c1-4a8e-48f6-f56f-d51488ce2f58",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1716
        }
      },
      "cell_type": "code",
      "source": [
        " # training \n",
        "\n",
        "\n",
        "model.train(datasets, steps_per_epoch = steps_per_epoch, epochs=25,validation_data=val_datasets, validation_steps=steps_per_valid_epoch)\n",
        "#model.model.save('/gdrive/My Drive/LibiumNet/libium-500-60-L3.h5')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training...\n",
            "Epoch 1/25\n",
            "20/20 [==============================] - 7s 365ms/step - loss: 0.6930 - acc: 0.5000\n",
            "100/100 [==============================] - 42s 421ms/step - loss: 0.6978 - acc: 0.5200 - val_loss: 0.6930 - val_acc: 0.5000\n",
            "Epoch 2/25\n",
            "20/20 [==============================] - 7s 366ms/step - loss: 0.6915 - acc: 0.5500\n",
            "100/100 [==============================] - 39s 390ms/step - loss: 0.6941 - acc: 0.4300 - val_loss: 0.6915 - val_acc: 0.5500\n",
            "Epoch 3/25\n",
            "20/20 [==============================] - 7s 356ms/step - loss: 0.6935 - acc: 0.5000\n",
            "100/100 [==============================] - 39s 386ms/step - loss: 0.6937 - acc: 0.5600 - val_loss: 0.6935 - val_acc: 0.5000\n",
            "Epoch 4/25\n",
            "20/20 [==============================] - 7s 351ms/step - loss: 0.6920 - acc: 0.5500\n",
            "100/100 [==============================] - 38s 384ms/step - loss: 0.6913 - acc: 0.5500 - val_loss: 0.6920 - val_acc: 0.5500\n",
            "Epoch 5/25\n",
            "20/20 [==============================] - 7s 351ms/step - loss: 0.6954 - acc: 0.4500\n",
            "100/100 [==============================] - 38s 383ms/step - loss: 0.6926 - acc: 0.5700 - val_loss: 0.6954 - val_acc: 0.4500\n",
            "Epoch 6/25\n",
            "20/20 [==============================] - 7s 353ms/step - loss: 0.6890 - acc: 0.6000\n",
            "100/100 [==============================] - 39s 386ms/step - loss: 0.6921 - acc: 0.5500 - val_loss: 0.6890 - val_acc: 0.6000\n",
            "Epoch 7/25\n",
            "20/20 [==============================] - 7s 354ms/step - loss: 0.6946 - acc: 0.4500\n",
            "100/100 [==============================] - 38s 379ms/step - loss: 0.6871 - acc: 0.6900 - val_loss: 0.6946 - val_acc: 0.4500\n",
            "Epoch 8/25\n",
            "20/20 [==============================] - 7s 359ms/step - loss: 0.6915 - acc: 0.5500\n",
            "100/100 [==============================] - 40s 396ms/step - loss: 0.6873 - acc: 0.6100 - val_loss: 0.6915 - val_acc: 0.5500\n",
            "Epoch 9/25\n",
            "20/20 [==============================] - 7s 361ms/step - loss: 0.6932 - acc: 0.5000\n",
            "100/100 [==============================] - 39s 387ms/step - loss: 0.6756 - acc: 0.5800 - val_loss: 0.6932 - val_acc: 0.5000\n",
            "Epoch 10/25\n",
            "20/20 [==============================] - 7s 370ms/step - loss: 0.7032 - acc: 0.3000\n",
            "100/100 [==============================] - 39s 393ms/step - loss: 0.6826 - acc: 0.6100 - val_loss: 0.7032 - val_acc: 0.3000\n",
            "Epoch 11/25\n",
            "20/20 [==============================] - 7s 360ms/step - loss: 0.6885 - acc: 0.6500\n",
            "100/100 [==============================] - 39s 386ms/step - loss: 0.6735 - acc: 0.6300 - val_loss: 0.6885 - val_acc: 0.6500\n",
            "Epoch 12/25\n",
            "20/20 [==============================] - 7s 362ms/step - loss: 0.6913 - acc: 0.5500\n",
            "100/100 [==============================] - 38s 384ms/step - loss: 0.6673 - acc: 0.6600 - val_loss: 0.6913 - val_acc: 0.5500\n",
            "Epoch 13/25\n",
            "20/20 [==============================] - 7s 359ms/step - loss: 0.6969 - acc: 0.4500\n",
            "100/100 [==============================] - 38s 384ms/step - loss: 0.6522 - acc: 0.7500 - val_loss: 0.6969 - val_acc: 0.4500\n",
            "Epoch 14/25\n",
            "20/20 [==============================] - 7s 373ms/step - loss: 0.7012 - acc: 0.4000\n",
            "100/100 [==============================] - 39s 390ms/step - loss: 0.6293 - acc: 0.7500 - val_loss: 0.7012 - val_acc: 0.4000\n",
            "Epoch 15/25\n",
            "20/20 [==============================] - 7s 356ms/step - loss: 0.6933 - acc: 0.5000\n",
            "100/100 [==============================] - 38s 384ms/step - loss: 0.6097 - acc: 0.7800 - val_loss: 0.6933 - val_acc: 0.5000\n",
            "Epoch 16/25\n",
            "20/20 [==============================] - 7s 362ms/step - loss: 0.6835 - acc: 0.6500\n",
            "100/100 [==============================] - 39s 385ms/step - loss: 0.6053 - acc: 0.8100 - val_loss: 0.6835 - val_acc: 0.6500\n",
            "Epoch 17/25\n",
            "20/20 [==============================] - 7s 365ms/step - loss: 0.7029 - acc: 0.4000\n",
            "100/100 [==============================] - 39s 391ms/step - loss: 0.5599 - acc: 0.8900 - val_loss: 0.7029 - val_acc: 0.4000\n",
            "Epoch 18/25\n",
            "20/20 [==============================] - 7s 362ms/step - loss: 0.6892 - acc: 0.6000\n",
            "100/100 [==============================] - 39s 388ms/step - loss: 0.6198 - acc: 0.6600 - val_loss: 0.6892 - val_acc: 0.6000\n",
            "Epoch 19/25\n",
            "20/20 [==============================] - 7s 361ms/step - loss: 0.6949 - acc: 0.3500\n",
            "100/100 [==============================] - 39s 390ms/step - loss: 0.6181 - acc: 0.7400 - val_loss: 0.6949 - val_acc: 0.3500\n",
            "Epoch 20/25\n",
            "20/20 [==============================] - 7s 352ms/step - loss: 0.6877 - acc: 0.6500\n",
            "100/100 [==============================] - 38s 383ms/step - loss: 0.5810 - acc: 0.7800 - val_loss: 0.6877 - val_acc: 0.6500\n",
            "Epoch 21/25\n",
            "20/20 [==============================] - 7s 358ms/step - loss: 0.6964 - acc: 0.4000\n",
            "100/100 [==============================] - 39s 389ms/step - loss: 0.5150 - acc: 0.8800 - val_loss: 0.6964 - val_acc: 0.4000\n",
            "Epoch 22/25\n",
            "20/20 [==============================] - 7s 363ms/step - loss: 0.6924 - acc: 0.5500\n",
            "100/100 [==============================] - 39s 389ms/step - loss: 0.5176 - acc: 0.8200 - val_loss: 0.6924 - val_acc: 0.5500\n",
            "Epoch 23/25\n",
            "20/20 [==============================] - 7s 355ms/step - loss: 0.6933 - acc: 0.5500\n",
            "100/100 [==============================] - 39s 392ms/step - loss: 0.4588 - acc: 0.8900 - val_loss: 0.6933 - val_acc: 0.5500\n",
            "Epoch 24/25\n",
            "20/20 [==============================] - 7s 366ms/step - loss: 0.6885 - acc: 0.6500\n",
            "100/100 [==============================] - 39s 388ms/step - loss: 0.4451 - acc: 0.9000 - val_loss: 0.6885 - val_acc: 0.6500\n",
            "Epoch 25/25\n",
            "20/20 [==============================] - 7s 358ms/step - loss: 0.6981 - acc: 0.3500\n",
            "100/100 [==============================] - 39s 387ms/step - loss: 0.3760 - acc: 0.9500 - val_loss: 0.6981 - val_acc: 0.3500\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEWCAYAAAB1xKBvAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzt3Xd8VHXa///XNemNkEYCBAi9Nwko\nAopKieiCvbviurLN1S1ff+LeW91b1731tuy97CpY17WuFRWlKIiIlIChhRYCQgiEEJKQENKv3x9n\nwAAhDGEmk3I9H495ZOaUmetkknnP+XzO+RxRVYwxxpgzcfm7AGOMMS2DBYYxxhiPWGAYY4zxiAWG\nMcYYj1hgGGOM8YgFhjHGGI9YYBhjjPGIBYYxjSAiu0Rkgr/rMKYpWWAYY4zxiAWGMV4kIneLSJaI\nHBKRuSLSyT1dRORJETkgIodFZIOIDHLPmyIimSJSIiJ7ReT/+XcrjKmfBYYxXiIilwJ/AW4AOgLf\nAm+4Z08CLgL6ANHuZQrc854HfqSqUcAg4PMmLNsYjwX6uwBjWpFbgRdUdS2AiDwIFIpIClAFRAH9\ngFWqurnOelXAABFZp6qFQGGTVm2Mh2wPwxjv6YSzVwGAqpbi7EV0VtXPgb8Ds4ADIjJbRNq5F70W\nmAJ8KyJfiMjoJq7bGI9YYBjjPblAt2MPRCQCiAP2Aqjq31R1BDAAp2nqfvf01ao6DegAvA+81cR1\nG+MRCwxjGi9IREKP3YDXgTtFZJiIhACPACtVdZeIjBSR80UkCDgClAO1IhIsIreKSLSqVgGHgVq/\nbZExDbDAMKbx5gFH69zGA78D3gH2AT2Bm9zLtgPm4PRPfIvTVPWYe97twC4ROQz8GKcvxJhmR+wC\nSsYYYzxhexjGGGM8YoFhjDHGIxYYxhhjPGKBYYwxxiOt5kzv+Ph4TUlJ8XcZxhjToqxZs+agqiZ4\nsmyrCYyUlBTS09P9XYYxxrQoIvLtmZdyWJOUMcYYj1hgGGOM8YgFhjHGGI9YYBhjjPGIBYYxxhiP\nWGAYY4zxiAWGMcYYj/g0MEQkTUS2ikiWiMysZ/6TIpLhvm0TkaI68+4Qke3u2x2+rNO0AhWlkPEa\n7Fvv70paL1XYuRQqy/xdifETn524JyIBOJejnAjkAKtFZK6qZh5bRlV/WWf5nwPD3fdjgT8AqYAC\na9zr2rWOzYnKDsHKZ2HVs3C0EFyBcPFMGPtLCGiB56WqwuG9ULADOo+AkEh/V+SoqYYP74WMVyG2\nB0z9O6SM8XdVpon58j9qFJClqtkAIvIGMA3IPM3yN+OEBMBkYKGqHnKvuxBIw7mimTFwOBe+ngXp\nL0LVEeg7Bc7/Eax9BRb/N2xfANc863y4NVeqULIPcr+B3Azn574MOJLvzA+KgAHTYNgt0G0MuPzU\nglxVDu/cBVs+gtQfwI7P4aUpkHoXTPgjhLY70zOYVsKXgdEZ2FPncQ5wfn0Likg3oDvweQPrdq5n\nvRnADICuXbs2qkhV5ZON++mTGElKXASBAdat06wV7ICvnoJ1b0BtDQy+Dsb8AhIHOPN7jIc+afDx\nr+GZcZD2KAy/DUT8WbWjZP93wXAsHErznHnigoT+0HsydBoG0V1g68ew8T1Y9xq07wpDb4ahNzVt\nCFaUwOs3w64v4fLH4PwZUHkEPn8YVvwDts2H7z0NvSc0XU3mO6qQvQSOHoJB1/r85ZrLPvtNwNuq\nWnM2K6nqbGA2QGpqaqMuHZh3uIKfvroWgOBAF707RNIvqR39kqLo1zGKfkntSIgKacxTG2/atx6W\nPQmZ74MrCIbfDmPuhZiUU5cdcj10vQDe/wnMvQe2fQrf+xtExDVdvapwKNv5Np69BPaucfYmwAmH\n+L7Q81LoNNy5JQ6C4PATn6NvGqT9FbZ87DQFffE/8MVfoeuFMOxmGHCVb7/dHzkIr17n/O6vmQND\nbnCmB0dA2iMw8Cr44B549VonzCY/AuGxvqvHm/ZvgJx0SOjr/O5b2l5STRVseg+W/83ZlsTBMPAa\nn38x8tklWkVkNPBHVZ3sfvwggKr+pZ5lvwF+pqrL3Y9vBsar6o/cj58FlqjqaZukUlNTtTGDD1bV\n1LItr4St+0vYsr+EzfsOs3V/CQdKKo4vExcRTL+OUfRNbOcOkSh6JEQSEugi0CWIv769Vh6Bg9vd\nt61wcBscKYCUsc6HTcdh3v8DqiiFnFUQGAbxvSE8zrd/pN8uhy+fgKyFEBwFI++CC34KUYlnXre2\nFlbMgs8egrAYmDYLek/0Xa1lh5xwyF4MO5ZA8W5nenRXJ8A6n+e8J0mDG9c3UZwD69+EjNehYLvz\nHgyY6nxYd78IXAHe25biHHjlaijaDde/7Pw91ae6ApY+5oR5WCxc8bjTjNZYh3Y6Heshkc7eljf7\ncKorIHMurH4O9qw4cV5Md+g4BJKGQMehzk9P/saaWkUJrP0XrPgnFO9xvnhc+HMnzAMb98VWRNao\naqpHy/owMAKBbcBlwF5gNXCLqm46abl+wKdAd3UX4+70XgOc515sLTDiWJ9GfRobGNRUOW9AeKzz\noRIWC+GxHKqNYEtBNVv2l7JlvxMiW/NKKK+qPeUpAlxCoEsICnARKRUkBhTTQZxbghQRRxHxFBHm\nqkXDYgiMiCE4Kp7I6DjaxSYQG59IRLt45/VDo0/srFV1vukd3OYOhe2Q7/557AMJUHFR2z6FmuAo\ngg5sQLSW2qiO1PaajPS9nICeF0NQWON+P3vXuD8Iv3DCorb6u/mh0RDXq55bT+ebqCcqy6DsoLOd\nRw667+fDlnnOP3Z4nBMSI38IYe3Pfhv2b4R374YDmc5zTPzzqd/mG6O6AnavcAfEYti3DlAIaed8\ngPcY7+xFxPbwbqiqOt+O170GG96BimJo1xmG3QqjZkCkRyNVn97B7fCvq6DiMNz8hmed2/vWwwc/\ng/3rof9UmPK4Zx+4RwudgNix2Pk9Fu76bl5gGPSZDIOugd6TGvf3C1C0B9a86PyfH8l33o/Uu5ym\ny0M7nNr3r3N+FtUZuDUy0R0gQ777GZEAVUehquykn8ful584r7oC4vtAl1FOs2Jj/w4O74OVzzh9\ndhXFTp/Whfc6v5dz7NtqFoHhLmQK8BQQALygqg+LyENAuqrOdS/zRyBUVWeetO4PgN+4Hz6sqi82\n9FqNDoySPPjfPvXPCwhxB4kTJhoWQ4mrHfnV4RRUBhBScYiwygLCKw8SXllARNUhQmpPPeSwFhcl\nAe2p0gDCa0sIp7zBkspd4VQGRaPBUYSW5xFSVXx8XoWEsj+oC7tdyWTTmW01ndhUmUhmRTyVBAEQ\ny2EucWVwWcBaLnKtJ1LKKdMQlutglkoqy1wjKAmMJcglBAW66BAVwsiUWEZ2j2VE1/a0K9nhDogl\nsGsZVJYC4rSt9xgPKeNAa6Egy/lwKchy+hYO55y4IVGdnOCI7+18oFUcdvaAjuR/FwpHCpxO6/pE\nd4UL73Gan871A76qHD7/M3z9d4jrDdfOcZqCPFVbCyW5zjfgfRnOB9y3y6H6qHNkVvJI6HEJ9LwE\nOp3XdEdoVZXD1nnOIcVZi5xvmcNvd751xnQ7++fLzYB/X+M0m932jvNt21M1VU4TyZK/Oh/uaY86\nfS51PySrK50vHccCIvcb528pONL5u+p5ifM3duQgbHoXMj9w/k6CI6Hv5U6zS6/LzvxturYWdi6B\nVc/Btk+ckO2TBqN+CD0uPf2H7NEiyNvoDpH1zs/8LXB2reX1i0xygqPL+c4eZ9IQCAxueJ0DW2D5\n/zl7lloD/b8HF94HySPOvR63ZhMYTanRgVFb63Q8Hi10Oo7KDp10/5DzR3T8fqFzv7bK+XYdmei+\ndajzM6nO40QndOo0F9RUVVBQcID8/DwKD+ZRUniQssMFVJYUUFNWCEeLCKs5TBRl5Gt7srQTO7QT\n+4K7cTQkkajwENqFBtIuLIh2oUG0Cwt0/wwiPDiAmlqluqaWqhqlprqcxIJ0Ug4upUfhUqIr86hF\nyAnvz6bIC8kIv5Csw4HEHljOaNnIWNdGOrhPhzkS2Q1Xz0sI63up8898pvbpyjKn3b4gy2kyKdjx\nXaiUFzkBHBHv7DFEJLjvxzs/j99PcPoaIhKcDwlvN3dlL4H3fgJHDsD4B53Db4+9N5Vlzjfcwl1Q\nuNP5ecj9s+hbqKn87nni+7r3IC5xmgBDorxbZ2Mc3A5fPe0cEKC1zgEBY38JHfp7tv6uZfDaTc6e\n7vffd8K+MfK3OX1He1ZCr4kw7tdOMGQvhl1fOV8QJMA5bLjnJU7QJqdCQNCpz1VTDd8ug43vwua5\nzv9fSDT0u8Lp5O1x8YnrHS10muxWP+fsPYTHwXnfhxF3Ni5AwQnlA5lOgJQXQ1C4+xZWz8+TponL\nWXfPStizyvl5bC8mIMRppjwWIsmjnL1DVfj2K/jqb7B9vrOnNfxWGP0znxzwYIHha6pOs0x9f+Be\nUlZZzf7icoICXLQLDSIyNJAA1zl+eKo63562fup8K81de8LsqpBYdrQbydLqgbx5sAc7qpyA6B4f\nwciUGEZ1j2NUSixdYsPOvt+m6igEhjaPo5WOFsJHv3K+wSYNdg5fLdz53RFLx4S0czrVY1Igtrv7\nfneno7RdJz8U7qHivc4hx2te+u6Q47G/dD6YTmfLPPjPdGcbb38Pok85KPHs1NbAqjnw2Z+cphmA\n2J7fBUT3cc4XrrNRU+UE/sZ3nUN8Kw47e//9v+fsdWQtgvX/cfb6kkc5zY8Dr2p0277PlOw/MUBy\nM5wvoOAEQlAE5G1wwm7UDBh5t08P2LDAMJ4p2e8cFll5xGlz7zDg+K56VU0tG/cWs3rXIVbtLCT9\n20MUlTl/1IntQogJD6ZWlVrF+Vnr3K+pVdQ9vUa/ux8a6OLOMd35/oXdCAn0YudsY6nChv/Asqec\nPaeYbk4YHA+H7s437eYQcI1VdghWzXbavo8WQrexTnD0uuzE7cp43el/6DgUbn3bux9Ohd86H4xd\nRjX+G359qisg6zMn9Ld+4jSbBoY5R8il3uU0n7YUVeVOM+exECnZ55x7M/QW7/S1nYEFhvG62lol\nK7+UlTsPsfbbQo5UVOMSIcAliHDqfRFcLhD3/eyDpXyVVUCX2DAeSOvHFYM7+u/osramohTWvgzL\n/+70wyQNcYJjwDQnUD6dCd0vhptebR5Na2er6ijkrHb2FsNi/F1Ni2OBYZqlpdvyeWTeZrbsL2F4\n1/b89or+jOjmveP2K6trCQrw42HOzV11pdN5+tVTTt9SVEfn22z/78G1zze/phvTJCwwTLNVU6u8\nvWYPjy/YRn5JBVcM7sj/l9aXbnEeHoJ7ksPlVSzKzGPehn0s3XaQ5Jgw7rgwhetGJBMR0lzOS21m\namucPoCvZznfytP+2jLH3TJeYYFhmr0jFdXMXprN7KXZVNfW8v3RKfz80l60Dz/DYYZA8dHvQuLL\n7QeprKmlU3QoEwcksi6nmIw9RUSFBHLDyC5MvzCFLrG+bwc2pqWywDAtRt7hcp5YsI231uyhXWgQ\nP7+0F7ePPrVjvPhoFQuPh0Q+VTVK5/ZhXD4oiSlDOjIsuT0u91Fka3cX8uJXu/hkwz5qVJnQP5E7\nx6QwukecNVcZcxILDNPibN53mEfmbebL7QfpGhvOzMv7MaZnPAsy9zNvwz6WZR08HhJTBicxZXBH\nhnVp32AA7C8u598rvuW1Vbs5dKSSfklR3DkmhWnDOhMa1AyO1DKmGbDAMC3Wkq0HeGTeZrbllSLi\nHP16LCSuGNKJocnRZ72XUF5Vw9yMXF74aidb9pcQGxHMzaO6cPsFKSRFh/poS4xpGSwwTItWXVPL\nu9/sZXdBGRMGJDYqJOqjqnydXcCLX+1i0eY8AkRIG5TEFYM7MrZ3PFGhvjsR05jmygLDmDPYXVDG\ny1/v4j/pezhcXk1QgDCqeyyX9O3Apf060COhmVzpzhgfs8AwxkPVNbWs+baQz7ce4PPNB9h+oBRw\nhkM5Fh6juscSHGgX1jKtkwWGMY2051AZi7ce4LPNB/g6u4DK6loiggMY1zuBS/t1YHy/BDpEWb+H\naT0sMIzxgrLKapZnFRzf+9h/2BmW/sKecTx6zRC6xtn5Habls8AwxstUlc37Svhscx6zv8xGFR6+\nehDThp3jqK7G+NnZBIY1zBrjARFhQKd2/Pyy3nxy3zj6JUVx3xsZ/OrNDErKq/xdnjFNwgLDmLOU\nHBPOGzMu4BcTevN+xl6u+Nsyvtld6O+yjPE5CwxjGiEwwMUvJvThrR+NpqZWuf6Zr5m1OIua2tbR\nxGtMfSwwjDkHqSmxzLtvHGmDknhs/lZufW4F+4qP+rssY3zCAsOYcxQdFsT/3Tycx64bwvqcYtKe\n+pJPN+7zd1nGeJ0FhjFeICJcn9qFj+8dR7e4cH7877U8+O4Gyiqr/V2aMV7j08AQkTQR2SoiWSIy\n8zTL3CAimSKySUReqzO9RkQy3Le5vqzTGG/pHh/B2z++kB9f3JM3Vu/me/+3jE25xf4uyxiv8Nl5\nGCISAGwDJgI5wGrgZlXNrLNMb+At4FJVLRSRDqp6wD2vVFU9HtDHzsMwzc1XWQf55ZsZFJVV8cg1\ng7luRLK/SzLmFM3lPIxRQJaqZqtqJfAGMO2kZe4GZqlqIcCxsDCmNRjTK55Pf3ERI7vH8P+9vc76\nNUyL58vA6AzsqfM4xz2trj5AHxH5SkRWiEhanXmhIpLunn5VfS8gIjPcy6Tn5+d7t3pjvCA2Ipg5\n309lWJf23Pt6Bst3HPR3ScY0mr87vQOB3sB44GZgjoi0d8/r5t5NugV4SkR6nryyqs5W1VRVTU1I\nSGiqmo05K+HBgbwwfSTd4sKZ8a81bNxrfRqmZfJlYOwFutR5nOyeVlcOMFdVq1R1J06fR28AVd3r\n/pkNLAGG+7BWY3yqfXgw/7prFNFhQdzxwip2Hjzi75KMOWu+DIzVQG8R6S4iwcBNwMlHO72Ps3eB\niMTjNFFli0iMiITUmT4GyMSYFqxjdBj/umsUCtz+/Ery3KPfGtNS+CwwVLUauAeYD2wG3lLVTSLy\nkIhMdS82HygQkUxgMXC/qhYA/YF0EVnnnv5o3aOrjGmpeiZE8tKdIyk8UskdL6yiuMwGLjQthw1v\nbowfLNt+kB+8tJohydG8ctf5hAUH+Lsk00Y1l8NqjTGnMbZ3PE/eOIw1uwu557W1VNXU+rskY87I\nAsMYP7liSEcemjaIz7YcYOY7G6i1kW5NMxfo7wKMactuv6Abh0oreXLRNuIig/nNlP7+LsmY07LA\nMMbP7r2sF4eOVDB7aTZxEcH86OJTTjkyplmwwDDGz0SEP3xvIIfKqvjLJ1uIiQjmhtQuZ17RmCZm\ngWFMM+ByCf97/VCKyiqZ+c56YsKDmTgg0d9lGXMC6/Q2ppkIDnTxzG0jGJzcnp+9tpYvttn4aKZ5\nscAwphmJCAnkxekj6ZUQyQ9fXs28DTbCrWk+LDCMaWZiI4J5fcYFDE1uzz2vreXN1bv9XZIxgAWG\nMc1SdFgQr9x1PuN6J/DAOxuYvXSHv0syxgLDmOYqLDiAOd9P5YohHXlk3hYem7+F1jKUj2mZ7Cgp\nY5qx4EAXf7tpOO1CA5m1eAfFR6t4aOogXC7xd2keqalVcouO0i4siHahgYi0jLpN/SwwjGnmAlzC\nI1cPpl1YEM9+kc3ho9X87w1DCQpo/g0E/1ySxeMLtgEQ6BJiIoKJDQ8mNqLhW+8OkQS2gO1rayww\njGkBRIQHL+9PdFgQ//PpVkorqpl1y3nNepRbVeXtNTkM6tyOq4Z15tCRyhNum/cd5lBZJUX1DPE+\nvm8CL04faXskzYwFhjEtyE/H9yI6LIjfvr+RO15YxXPTU2kXGuTvsuq1PqeYXQVl/PXawdw4sutp\nl6uuqaXoaBWHjlRSUFrJkq0HeHZpNos2H7CTF5sZ2+czpoW59fxuPH3TcNbuLuTm2SsoKK3wd0n1\n+iAjl+AAF2kDOza4XGCAi/jIEPokRjG6Zxz/b3JfeiZE8Mi8zVRW27DvzYkFhjEt0NShnZjz/VSy\nDpRy/bNfs7foqL9LOkFNrfLR+lwu7ptAdPjZ7QEFBbj47RUD2HnwCK+s+NZHFZrGsMAwpoW6pF8H\nXrnrfPIPV3D9P5eTnV/q75KOW5ldwIGSCqYN69So9cf3TWBc73ieXrSNwiOVXq7ONJYFhjEt2Kju\nsbw+4wIqqmuZ9ORSvvd/y/jt+xv4T/oetueV+O2iTB9k5BIRHMBl/RrXByEi/PaKAZRWVPP0Z9u9\nXJ1pLOv0NqaFG9Q5mvd+OobXV+9m3Z4i3v8ml3+vcIYTiQwJZEhyNEO7tGdocnuGd21PYrtQn9ZT\nUV3DJxv3MWlg0jkdxdU3KYqbRnXl3yu+5fbR3eiZEOnFKk1jWGAY0wp0jQvngbR+ANTWKtkHS/lm\ndxHrcopYt6eYOUuzqXbvbSS1C2VoFydErjsvmQ5eDpAvtuZzuLyaqY1sjqrrVxP78GFGLo98vJnn\np4/0QnXmXPg0MEQkDXgaCACeU9VH61nmBuCPgALrVPUW9/Q7gN+6F/tvVX3Zl7Ua01q4XEKvDlH0\n6hDF9e4LMZVX1bAp9zDr9hwLkSLmb8pjYWYe7/7kQq+e7zB3XS6xEcGM7RV/zs8VHxnCzy7txaOf\nbGHZ9oOM7X3uz2kaz2eBISIBwCxgIpADrBaRuaqaWWeZ3sCDwBhVLRSRDu7pscAfgFScIFnjXrfQ\nV/Ua05qFBgUwolsMI7rFHJ/20lc7+eOHmazeVcio7rFeeZ3SimoWbc7juhHJXjsTffqFKby68lv+\n++NMPr53HAEtZFiU1siXnd6jgCxVzVbVSuANYNpJy9wNzDoWBKp6wD19MrBQVQ+55y0E0nxYqzFt\nzo0juxITHsSzX3hvJNyFmfspr6pl2rDOXnvO0KAAZqb1Z8v+Et5cvcdrz2vOni8DozNQ993NcU+r\nqw/QR0S+EpEV7iYsT9c1xpyDsOAA7rgwhc+2HGDr/hKvPOfcjFw6tw9jRNeYMy98FqYMTmJkSgxP\nLNxKSfmpQ4mYpuHvw2oDgd7AeOBmYI6ItPd0ZRGZISLpIpKen2+XszTmbN0xOoWwoABmL80+5+cq\nKK1g6faDXDm0o9dH0z12mO3B0kr+scSuDeIvvgyMvUCXOo+T3dPqygHmqmqVqu4EtuEEiCfroqqz\nVTVVVVMTEhK8WrwxbUFMRDA3juzCBxl72Vd8bmeLz9u4n5paZdpQ3zQGDO3SnmuGd+b5ZTvZc6jM\nJ69hGubLwFgN9BaR7iISDNwEzD1pmfdx9i4QkXicJqpsYD4wSURiRCQGmOSeZozxsrvGdkeB57/c\neU7P82FGLr07RNK/Y5R3CqvH/Wl9cQk8+ukWn72GOT2fBYaqVgP34HzQbwbeUtVNIvKQiEx1LzYf\nKBCRTGAxcL+qFqjqIeDPOKGzGnjIPc0Y42VdYsO5ckhHXl+1m+J6hhr3xN6io6zadYipQzv5dEjy\njtFhzLioJx+v30f6LvtIaGo+7cNQ1Xmq2kdVe6rqw+5pv1fVue77qqq/UtUBqjpYVd+os+4LqtrL\nfXvRl3Ua09bNuKgHRypr+PfKxg329+G6XACvnKx3Jj++uAeJ7UL480eZfhv6pK3yd6e3MaYZGNgp\nmov6JPDiV7sor6o56/XnZuQyrEt7usVF+KC6E4UHB3L/5H6syynmg3WndG0aH7LAMMYA8OOLenCw\ntIJ3157dh3DWgRIy9x1m6lDf710cc83wzgzuHM3/fLqVo5VnH3CmcSwwjDEAjO4Zx5DkaOZ8mU3N\nWTT1zM3IxSVw5ZCGL5TkTS6X8LsrB7CvuJw5X577IcHGMxYYxhjAOdfhRxf1ZOfBIyzYtN+jdVSV\nD9blMrpnnNcHMTyTUd1juXxQEv9csoO8w+VN+tptlQWGMea4tEFJdIsL55kvdqB65r2MdTnFfFtQ\n5rNzL85k5uX9qKlVHpu/1S+v39ZYYBhjjgtwCXeP68G6nGJW7jzzYatz3dftnjwoqQmqO1W3uAim\nj0nhnbU5bMot9ksNbYkFhjHmBNeNSCY+MphnzjAoYU2t8uH6XMb3TSA67Oyu2+1NP7ukF0EBLt5Z\nY0dM+ZoFhjHmBKFBAUy/MIUlW/PZvO/waZdbkV1AfkmFV0embYzosCDG9Ypn/qb9HjWjmcazwDDG\nnOK2C7oRHhzAnAYGJZx77Lrd/Ts0YWX1mzwwib1FR9mUe/qAM+fOAsMYc4r24cHcPKorc9flsrfo\n1EEJK6prmLdxH5MHJhEa1PjrdnvLZf074BKY7+HRXaZxLDCMMfX6wdjuQP2DEi7Zmk+Jl67b7Q1x\nkSGM6h5rgeFjFhjGmHp1bh/G1KGdeGP1borKKk+YN3ddLnERwYzxwnW7vWXywCS25ZWSnV/q71Ja\nLQsMY8xpzbi4B2WVNbzy9XeDEpZWVLMoM48pgzt67brd3jB5oHNo7/xNeX6upPVqPu+2MabZ6ZfU\njkv6JvDS8u8GJVyYuZ+K6lqmNZPmqGM6tQ9jSHI0n1qzlM9YYBhjGvTji3tScKSS/6zJAeAD93W7\nz/Pydbu9YfLAJNbtKWJ/sQ0V4gsWGMaYBo3qHsuwLu2ZszSbAyXlfLn9IN8b2snr1+32hmPNUgsy\nbS/DFywwjDENEhF+fHEPdh8q41dvrnOu293MmqOO6dUhkp4JEXy60QLDFywwjDFnNHFAEj3iI1iW\ndZA+iZH0S/LddbvPVdqgJFbuPEThkcozL2zOigWGMeaMAlzC3Rf1APD5dbvP1eSBSdTUKos229FS\n3hbo7wKMMS3DteclU1JexY0ju/q7lAYN7hxNp+hQ5m/K4/rULv4up1WxPQxjjEeCA13MuKinX0em\n9YSIMGlgEku353Okotrf5bQqFhjGmFZn8sAkKqtr+WJbvr9LaVV8GhgikiYiW0UkS0Rm1jN/uojk\ni0iG+/bDOvNq6kyf68s6jTGty8iUGGIjgm1sKS/zWR+GiAQAs4CJQA6wWkTmqmrmSYu+qar31PMU\nR1V1mK/qM8a0XoEBLib078AnG/ZTWV1LcKA1pniDL3+Lo4AsVc1W1UrgDWCaD1/PGGOOSxuURElF\nNct3HPR3Ka2GR4EhIveJSDux8KC+AAAaxElEQVRxPC8ia0Vk0hlW6wzsqfM4xz3tZNeKyHoReVtE\n6h7SECoi6SKyQkSuOk1dM9zLpOfnW1ulMeY7F/aMJyI4wJqlvMjTPYwfqOphYBIQA9wOPOqF1/8Q\nSFHVIcBC4OU687qpaipwC/CUiPQ8eWVVna2qqaqampCQ4IVyjDGtRWhQAOP7dWBhZh41tXbpVm/w\nNDCOnaUzBXhFVTfVmXY6e4G6ewzJ7mnHqWqBqla4Hz4HjKgzb6/7ZzawBBjuYa3GGANA2sAkDpZW\nsnZ3ob9LaRU8DYw1IrIAJzDmi0gUUHuGdVYDvUWku4gEAzcBJxztJCId6zycCmx2T48RkRD3/Xhg\nDHByZ7kxxjRofN8EggNcNraUl3gaGHcBM4GRqloGBAF3NrSCqlYD9wDzcYLgLVXdJCIPichU92L3\nisgmEVkH3AtMd0/vD6S7py8GHq3n6CpjjGlQVGgQY3vHM3/TflStWepceXpY7WggQ1WPiMhtwHnA\n02daSVXnAfNOmvb7OvcfBB6sZ73lwGAPazPGmNOaPDCRz7ccYFPuYQZ1jvZ3OS2ap3sY/wTKRGQo\n8GtgB/Avn1VljDFeMqF/Ii6BBXa01DnzNDCq1dmfmwb8XVVnAc13fGNjjHGLiwxhZEqsXevbCzwN\njBIReRDncNqPRcSF049hjDHN3uSBSWzNK2HnwSP+LqVF8zQwbgQqcM7H2I9ziOxjPqvKGGO8aPIg\n59KtdhLfufEoMNwh8SoQLSJXAuWqan0YxpgWoXP7MAZ3jrbDa8+Rp0OD3ACsAq4HbgBWish1vizM\nGGO8afLARDL2FLG/uNzfpbRYnjZJ/RfOORh3qOr3cQYW/J3vyjLGGO9KczdLLci0vYzG8jQwXKp6\noM7jgrNY1xhj/K5Xhyh6JERYP8Y58PRD/1MRme++4NF04GNOOiHPGGOau7SBSazIPkRRWaW/S2mR\nPO30vh+YDQxx32ar6gO+LMwYY7xt8sAkamqVRZsPnHlhcwqPr7inqu8A7/iwFmOM8akhydF0jA5l\n/qb9XDci2d/ltDgNBoaIlAD1jdglgKpqO59UZYwxPiAiTB6YxOurdlNWWU14sM+uUt0qNdgkpapR\nqtqunluUhYUxpiWaNDCRiupavth65qt0qioHSsr5ZnchH63PZUd+aRNU2HxZvBpj2pRRKbHEhAcx\nf9N+Jg1MIu9wOXuLjpJTWMbewqPu+0fZW3iUnKKjVFZ/d+mfPomRzP/FRYic6fpxrZMFhjGmTQkM\ncDGhfyLvrM3ho/X7qD7p8q3xkSF0jgmjf8d2TByQSOeYMDq3D2N9TjFPf7adzH2HGdipbQ6TboFh\njGlz7r6oBwAd2oXQuX04yTFhx4MhNCig3nXO6xrDrMVZfJCRa4FhjDFtRZ/EKB67fuhZrRMTEcz4\nvh34IGMvD6T1I8DV9pql7GxtY4zx0FXDO5F3uIKV2QX+LsUvLDCMMcZDE/onEhkSyHvf7PV3KX5h\ngWGMMR4KDQogbVASn27cT3lVjb/LaXIWGMYYcxauGtaZkopqPmuDw4v4NDBEJE1EtopIlojMrGf+\ndBHJF5EM9+2HdebdISLb3bc7fFmnMcZ4anTPODpEhfB+RttrlvLZUVIiEgDMAiYCOcBqEZmrqpkn\nLfqmqt5z0rqxwB+AVJyhSda41y30Vb3GGOOJAJcwdWgnXv56F0VllbQPD/Z3SU3Gl3sYo4AsVc1W\n1UrgDWCah+tOBhaq6iF3SCwE0nxUpzHGnJWrhnemqkb5eMM+f5fSpHwZGJ2BPXUe57innexaEVkv\nIm+LSJezWVdEZohIuoik5+efeVwYY4zxhoGd2tGrQyQffJPr71KalL87vT8EUlR1CM5exMtns7Kq\nzlbVVFVNTUhI8EmBxhhzMhHh6uGdWbXrEDmFZf4up8n4MjD2Al3qPE52TztOVQtUtcL98DlghKfr\nGmOMP00d2gmADzLazl6GLwNjNdBbRLqLSDBwEzC37gIi0rHOw6nAZvf9+cAkEYkRkRhgknuaMcY0\nC11iw0ntFsP73+xFtb7LBrU+PgsMVa0G7sH5oN8MvKWqm0TkIRGZ6l7sXhHZJCLrgHuB6e51DwF/\nxgmd1cBD7mnGGNNsXDW8M9sPlJK577C/S2kS0lqSMTU1VdPT0/1dhjGmDSk8UsnIhxdx55gU/uuK\nAf4up1FEZI2qpnqyrL87vY0xpsU6NoLt3HW51NS2ji/fDbHAMMaYc9CWRrC1wDDGmHPQlkawtcAw\nxphz0JZGsLXAMMaYc9RWRrC1wDDGmHPUVkawtcAwxphzdGwE2yVbD1BUVunvcnzGAsMYY7ygLYxg\na4FhjDFe0BZGsLXAMMYYL2gLI9haYBhjjJe09hFsLTCMMcZLWvsIthYYxhjjRa15BFsLDGOM8aIr\nBnck0CW83wqHCrHAMMYYL2rNI9haYBhjjJcdG8F2RSsbwdYCwxhjvOzYCLatrVnKAsMYY7ystY5g\na4FhjDE+cGwE2ycWbqOqptbf5XiFBYYxxvjA6J5xXD28M7OXZjPt71+xcW+xv0s6ZxYYxhjjAwEu\n4ckbh/Hs7SPIL61g2qyveGz+lhbdROXTwBCRNBHZKiJZIjKzgeWuFREVkVT34xQROSoiGe7bM76s\n0xhjfGXywCQW/fJirh7emVmLd3Dl/y1j7e5Cf5fVKD4LDBEJAGYBlwMDgJtFZEA9y0UB9wErT5q1\nQ1WHuW8/9lWdxhjja9HhQTx+/VBe/sEoyiqqufafy/nzR5kcrWxZexu+3MMYBWSparaqVgJvANPq\nWe7PwF+Bch/WYowxfndxnwTm//Iibj2/K88v28nkp5ayfMdBf5flMV8GRmdgT53HOe5px4nIeUAX\nVf24nvW7i8g3IvKFiIyr7wVEZIaIpItIen5+vtcKN8YYX4kKDeK/rxrMGzMuQARumbOS/3pvAyXl\nVf4u7Yz81uktIi7gCeDX9czeB3RV1eHAr4DXRKTdyQup6mxVTVXV1ISEBN8WbIwxXnRBjzg+ve8i\n7h7XnddX7Wbyk0tZvPWAv8tqkC8DYy/Qpc7jZPe0Y6KAQcASEdkFXADMFZFUVa1Q1QIAVV0D7AD6\n+LBWY4xpcmHBAfzXFQN4+ycXEh4SyJ0vruaBt9dT20zHoPJlYKwGeotIdxEJBm4C5h6bqarFqhqv\nqimqmgKsAKaqarqIJLg7zRGRHkBvINuHtRpjjN+c1zWGj+8dy/QLU3gzfU+zPYrKZ4GhqtXAPcB8\nYDPwlqpuEpGHRGTqGVa/CFgvIhnA28CPVfWQr2o1xhh/CwkM4FeT+hDoEhZuzvN3OfWS1nJVqNTU\nVE1PT/d3GcYYc05ue24l+4qP8tmvxzfJ64nIGlVN9WRZO9PbGGOakQn9O7Aj/wg7Dx7xdymnsMAw\nxphm5LL+iQB81gybpSwwjDGmGekSG06/pCgWZlpgGGOMOYMJ/RNJ/7aQwiOV/i7lBBYYxhjTzEwY\nkEhNrbJkW/M6kc8CwxhjmpkhnaNJiAph0WYLDGOMMQ1wuYQJ/TvwxdZ8Kqubz9X6LDCMMaYZuqxf\nIqUV1azcWeDvUo6zwDDGmGZoTK94QoNcLGpGR0tZYBhjTDMUFhzA2F4JLNp8gOYyIocFhjHGNFMT\nB3Rgb9FRtuwv8XcpAAT6uwBfqqqqIicnh/Jyu5ifv4SGhpKcnExQUJC/SzGmxbmkXwcAFmXm0b/j\nKZcEanKtOjBycnKIiooiJSUFEfF3OW2OqlJQUEBOTg7du3f3dznGtDgdokIZ1qU9izbn8fPLevu7\nnNbdJFVeXk5cXJyFhZ+ICHFxcbaHZ8w5mDggkXU5xeQd9v//UasODMDCws/s92/MuZlwfDBC/5/E\n1+oDwxhjWrI+iZEkx4Q1i9FrLTB8qKioiH/84x+NWnfKlCkUFRU1uMzvf/97Fi1a1KjnP1lKSgoH\nDx70ynMZY7xHRJjQP5FlWQcpq6z2ay0WGD7UUGBUVzf8xs+bN4/27ds3uMxDDz3EhAkTGl2fMaZl\nmDggkYrqWpZt9++XulZ9lFRdf/pwE5m5h736nAM6teMP3xt42vkzZ85kx44dDBs2jIkTJ3LFFVfw\nu9/9jpiYGLZs2cK2bdu46qqr2LNnD+Xl5dx3333MmDEDcL7xp6enU1payuWXX87YsWNZvnw5nTt3\n5oMPPiAsLIzp06dz5ZVXct1115GSksIdd9zBhx9+SFVVFf/5z3/o168f+fn53HLLLeTm5jJ69GgW\nLlzImjVriI+PP23dTzzxBC+88AIAP/zhD/nFL37BkSNHuOGGG8jJyaGmpobf/e533HjjjcycOZO5\nc+cSGBjIpEmTePzxx736OzbGwKjusUSFBrJocx6TBib5rY42Exj+8Oijj7Jx40YyMjIAWLJkCWvX\nrmXjxo3HDzN94YUXiI2N5ejRo4wcOZJrr72WuLi4E55n+/btvP7668yZM4cbbriBd955h9tuu+2U\n14uPj2ft2rX84x//4PHHH+e5557jT3/6E5deeikPPvggn376Kc8//3yDNa9Zs4YXX3yRlStXoqqc\nf/75XHzxxWRnZ9OpUyc+/vhjAIqLiykoKOC9995jy5YtiMgZm9CMMY0TFOBifN8OfL7lALW1isvl\nn4NJ2kxgNLQn0JRGjRp1wjkJf/vb33jvvfcA2LNnD9u3bz8lMLp3786wYcMAGDFiBLt27ar3ua+5\n5prjy7z77rsALFu27Pjzp6WlERMT02B9y5Yt4+qrryYiIuL4c3755ZekpaXx61//mgceeIArr7yS\ncePGUV1dTWhoKHfddRdXXnklV1555Vn+NowxnprQvwMfrsslI6eI87o2/H/sKz7twxCRNBHZKiJZ\nIjKzgeWuFREVkdQ60x50r7dVRCb7ss6mdOyDGJw9jkWLFvH111+zbt06hg8fXu85CyEhIcfvBwQE\nnLb/49hyDS3TWH369GHt2rUMHjyY3/72tzz00EMEBgayatUqrrvuOj766CPS0tK8+prGmO+M79OB\nAJf4dTBCnwWGiAQAs4DLgQHAzSIyoJ7looD7gJV1pg0AbgIGAmnAP9zP16JERUVRUnL6MWCKi4uJ\niYkhPDycLVu2sGLFCq/XMGbMGN566y0AFixYQGFhYYPLjxs3jvfff5+ysjKOHDnCe++9x7hx48jN\nzSU8PJzbbruN+++/n7Vr11JaWkpxcTFTpkzhySefZN26dV6v3xjjiA4PYlRKLIv8eHitL5ukRgFZ\nqpoNICJvANOAzJOW+zPwV+D+OtOmAW+oagWwU0Sy3M/3tQ/r9bq4uDjGjBnDoEGDuPzyy7niiitO\nmJ+WlsYzzzxD//796du3LxdccIHXa/jDH/7AzTffzCuvvMLo0aNJSkoiKirqtMufd955TJ8+nVGj\nRgFOp/fw4cOZP38+999/Py6Xi6CgIP75z39SUlLCtGnTKC8vR1V54oknvF6/MeY7EwYk8uePMtld\nUEbXuPCmL0BVfXIDrgOeq/P4duDvJy1zHvCO+/4SINV9/+/AbXWWex64rp7XmAGkA+ldu3bVk2Vm\nZp4yra0pLy/XqqoqVVVdvny5Dh06tMlrsPfBGO/YdbBUuz3wkT7/ZbbXnhNIVw8/1/3W6S0iLuAJ\nYHpjn0NVZwOzAVJTU5vHgPHNzO7du7nhhhuora0lODiYOXPm+LskY0wjdYuLoHeHSBZtzuMHY5t+\nQE9fBsZeoEudx8nuacdEAYOAJe7xhpKAuSIy1YN1jYd69+7NN9984+8yjDFeMmFAIrOXZlNcVkV0\neNNeNsCXR0mtBnqLSHcRCcbpxJ57bKaqFqtqvKqmqGoKsAKYqqrp7uVuEpEQEekO9AZW+bBWY4xp\nESb0T6SmVlmyrekHI/RZYKhqNXAPMB/YDLylqptE5CH3XkRD624C3sLpIP8U+Jmq1viqVmOMaSmG\ndWlPfGQwi/wweq1P+zBUdR4w76Rpvz/NsuNPevww8LDPijPGmBYowCVc0rcDn27aT1VNLUEBTTck\noA0+aIwxLcyEAYmUlFezeuehJn1dCwwfOpfhzQGeeuopysrKjj/2ZMhzT+zatYtBgwad8/MYY/xj\nXO94ggNdLGzik/gsMHzI24HhyZDnxpjWLzw4kLG94lm0Oe/YOWlNos0MPsgnM2H/Bu8+Z9JguPzR\n084+eXjzxx57jMcee4y33nqLiooKrr76av70pz/VO3R4Xl4eubm5XHLJJcTHx7N48WKPhjxfvXo1\nd911Fy6Xi4kTJ/LJJ5+wcePG09ZYXl7OT37yE9LT0wkMDOSJJ57gkksuYdOmTdx5551UVlZSW1vL\nO++8Q6dOneod4twY0/Qu6++MXrstr5S+SacfvcGb2k5g+MHJw5svWLCA7du3s2rVKlSVqVOnsnTp\nUvLz808ZOjw6OponnniCxYsX13vtitMNeX7nnXcyZ84cRo8ezcyZpx3v8bhZs2YhImzYsIEtW7Yw\nadIktm3bxjPPPMN9993HrbfeSmVlJTU1NcybN++UOo0x/nFZv0T+i40s2pxngeF1DewJNJUFCxaw\nYMEChg8fDkBpaSnbt29n3Lhxpwwdfib1DXleVFRESUkJo0ePBuCWW27ho48+avB5li1bxs9//nMA\n+vXrR7du3di2bRujR4/m4YcfJicnh2uuuYbevXszePDgs67TGOMbSdGhDEmOZtHmPH52Sa8meU3r\nw2hCqsqDDz5IRkYGGRkZZGVlcdddd9U7dPiZeDrkeWPdcsstzJ07l7CwMKZMmcLnn3/eqDqNMb4z\noX8iGXuKOFBy6mURfMECw4dOHt588uTJvPDCC5SWlgKwd+9eDhw4UO/Q4fWtfybt27cnKiqKlSud\nkeLfeOONM64zbtw4Xn31VQC2bdvG7t276du3L9nZ2fTo0YN7772XadOmsX79+tPWaYzxj8v6d0AV\nFm9pmpP42k6TlB+cPLz5Y489xubNm483GUVGRvLvf/+brKysU4YOB5gxYwZpaWl06tSJxYsXe/Sa\nzz//PHfffTcul4uLL76Y6OjoBpf/6U9/yk9+8hMGDx5MYGAgL730EiEhIbz11lu88sorBAUFkZSU\nxG9+8xtWr15db53GGP8Y0LEdnaJDWZh5gBtHdvX560lTHpLlS6mpqZqenn7CtM2bN9O/f38/VeQf\npaWlREZGAk6n+759+3j66af9WlNbfB+MaSp//3w7R6tquH9yv0atLyJrVDX1zEvaHkar8/HHH/OX\nv/yF6upqunXrxksvveTvkowxPnTPpb2b7LUsMFqZG2+80c6NMMb4RKvv9G4tTW4tlf3+jWk9WnVg\nhIaGUlBQYB9afqKqFBQUEBoa6u9SjDFe0KqbpJKTk8nJySE/P9/fpbRZoaGhJCcn+7sMY4wXtOrA\nCAoKonv3pr/urTHGtEatuknKGGOM91hgGGOM8YgFhjHGGI+0mjO9RSQf+PYcniIeOOilcloa2/a2\nqy1vf1vedvhu+7upaoInK7SawDhXIpLu6enxrY1te9vcdmjb29+Wtx0at/3WJGWMMcYjFhjGGGM8\nYoHxndn+LsCPbNvbrra8/W1526ER2299GMYYYzxiexjGGGM8YoFhjDHGI20+MEQkTUS2ikiWiMz0\ndz1NTUR2icgGEckQkfQzr9FyicgLInJARDbWmRYrIgtFZLv7Z4w/a/Sl02z/H0Vkr/v9zxCRKf6s\n0VdEpIuILBaRTBHZJCL3uae3+ve/gW0/6/e+TfdhiEgAsA2YCOQAq4GbVTXTr4U1IRHZBaSqaqs/\ngUlELgJKgX+p6iD3tP8BDqnqo+4vDDGq+oA/6/SV02z/H4FSVX3cn7X5moh0BDqq6loRiQLWAFcB\n02nl738D234DZ/net/U9jFFAlqpmq2ol8AYwzc81GR9R1aXAoZMmTwNedt9/GecfqVU6zfa3Caq6\nT1XXuu+XAJuBzrSB97+BbT9rbT0wOgN76jzOoZG/yBZMgQUiskZEZvi7GD9IVNV97vv7gUR/FuMn\n94jIeneTVatrkjmZiKQAw4GVtLH3/6Rth7N879t6YBgYq6rnAZcDP3M3W7RJ6rTPtrU22n8CPYFh\nwD7gf/1bjm+JSCTwDvALVT1cd15rf//r2fazfu/bemDsBbrUeZzsntZmqOpe988DwHs4zXRtSZ67\njfdYW+8BP9fTpFQ1T1VrVLUWmEMrfv9FJAjnA/NVVX3XPblNvP/1bXtj3vu2Hhirgd4i0l1EgoGb\ngLl+rqnJiEiEuxMMEYkAJgEbG16r1ZkL3OG+fwfwgR9raXLHPizdrqaVvv8iIsDzwGZVfaLOrFb/\n/p9u2xvz3rfpo6QA3IeSPQUEAC+o6sN+LqnJiEgPnL0KcC7X+1pr3n4ReR0YjzOscx7wB+B94C2g\nK87w+DeoaqvsGD7N9o/HaZJQYBfwozpt+q2GiIwFvgQ2ALXuyb/Bactv1e9/A9t+M2f53rf5wDDG\nGOOZtt4kZYwxxkMWGMYYYzxigWGMMcYjFhjGGGM8YoFhjDHGIxYYxviRiIwXkY/8XYcxnrDAMMYY\n4xELDGM8ICK3icgq93UDnhWRABEpFZEn3dcY+ExEEtzLDhORFe5B3d47NqibiPQSkUUisk5E1opI\nT/fTR4rI2yKyRURedZ+Zi4g86r6GwXoRadXDj5uWwQLDmDMQkf7AjcAYVR0G1AC3AhFAuqoOBL7A\nOXMa4F/AA6o6BOfs2mPTXwVmqepQ4EKcAd/AGT30F8AAoAcwRkTicIZrGOh+nv/27VYac2YWGMac\n2WXACGC1iGS4H/fAGWbhTfcy/wbGikg00F5Vv3BPfxm4yD1mV2dVfQ9AVctVtcy9zCpVzXEPApcB\npADFQDnwvIhcAxxb1hi/scAw5swEeFlVh7lvfVX1j/Us19hxdirq3K8BAlW1Gmf00LeBK4FPG/nc\nxniNBYYxZ/YZcJ2IdIDj14HuhvP/c517mVuAZapaDBSKyDj39NuBL9xXOssRkavczxEiIuGne0H3\ntQuiVXUe8EtgqC82zJizEejvAoxp7lQ1U0R+i3NlQhdQBfwMOAKMcs87gNPPAc4w2c+4AyEbuNM9\n/XbgWRF5yP0c1zfwslHAByISirOH8ysvb5YxZ81GqzWmkUSkVFUj/V2HMU3FmqSMMcZ4xPYwjDHG\neMT2MIwxxnjEAsMYY4xHLDCMMcZ4xALDGGOMRywwjDHGeOT/B2eFg6sYj9nzAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "g7pFUnVHKtiF",
        "colab_type": "code",
        "outputId": "66a26818-64e7-4f7a-dff3-5887757bc188",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        }
      },
      "cell_type": "code",
      "source": [
        "model.model.save('/gdrive/My Drive/LibiumNet/libium-100-20-LSTM2.h5')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-6ffed9f2460b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/gdrive/My Drive/LibiumNet/libium-100-20-LSTM2.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/network.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, filepath, overwrite, include_optimizer)\u001b[0m\n\u001b[1;32m   1332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msave_model\u001b[0m  \u001b[0;31m# pylint: disable=g-import-not-at-top\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m     \u001b[0msave_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minclude_optimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0msave_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/saving.py\u001b[0m in \u001b[0;36msave_model\u001b[0;34m(model, filepath, overwrite, include_optimizer)\u001b[0m\n\u001b[1;32m    150\u001b[0m           \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m             param_dset = optimizer_weights_group.create_dataset(\n\u001b[0;32m--> 152\u001b[0;31m                 name, val.shape, dtype=val.dtype)\n\u001b[0m\u001b[1;32m    153\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m               \u001b[0;31m# scalar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/h5py/_hl/group.py\u001b[0m in \u001b[0;36mcreate_dataset\u001b[0;34m(self, name, shape, dtype, data, **kwds)\u001b[0m\n\u001b[1;32m    117\u001b[0m             \u001b[0mdset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdsid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mdset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/h5py/_hl/group.py\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, name, obj)\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mHLObject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 287\u001b[0;31m                 \u001b[0mh5o\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlink\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlcpl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlcpl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    288\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSoftLink\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mh5py/h5o.pyx\u001b[0m in \u001b[0;36mh5py.h5o.link\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Unable to create link (name already exists)"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "hoAGsgNL7hC5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "gen = GenerateDataset('/gdrive/My Drive/LibiumNet/Lipread_mp4/', 'train')\n",
        "#gen = GenerateDataset('/gdrive/My Drive/LibiumNet/overfit_test/', 'train')\n",
        "datasets = gen.generator()\n",
        "num_samples = gen.get_sample_size()\n",
        "\n",
        "# validation\n",
        "val_gen = GenerateDataset('/gdrive/My Drive/LibiumNet/Lipread_mp4/', 'val')\n",
        "#val_gen = val_gen.generator()\n",
        "#val_gen = GenerateDataset('/gdrive/My Drive/LibiumNet/overfit_test/', 'test')\n",
        "val_gen = val_gen.generator()\n",
        "\n",
        "model.train(datasets, steps_per_epoch = steps_per_epoch, epochs=10,validation_data=val_gen, validation_steps=steps_per_epoch)\n",
        "model.model.save('/gdrive/My Drive/LibiumNet/libium-0-4.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DEi_OTFs7rUa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "gen = GenerateDataset('/gdrive/My Drive/LibiumNet/Lipread_mp4/', 'train')\n",
        "#gen = GenerateDataset('/gdrive/My Drive/LibiumNet/overfit_test/', 'train')\n",
        "datasets = gen.generator()\n",
        "num_samples = gen.get_sample_size()\n",
        "\n",
        "# validation\n",
        "val_gen = GenerateDataset('/gdrive/My Drive/LibiumNet/Lipread_mp4/', 'val')\n",
        "#val_gen = val_gen.generator()\n",
        "#val_gen = GenerateDataset('/gdrive/My Drive/LibiumNet/overfit_test/', 'test')\n",
        "val_gen = val_gen.generator()\n",
        "\n",
        "model.train(datasets, steps_per_epoch = steps_per_epoch, epochs=10,validation_data=val_gen, validation_steps=steps_per_epoch)\n",
        "model.model.save('/gdrive/My Drive/LibiumNet/libium-0-5.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Yy9Urc_b7tj_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "gen = GenerateDataset('/gdrive/My Drive/LibiumNet/Lipread_mp4/', 'train')\n",
        "#gen = GenerateDataset('/gdrive/My Drive/LibiumNet/overfit_test/', 'train')\n",
        "datasets = gen.generator()\n",
        "num_samples = gen.get_sample_size()\n",
        "\n",
        "# validation\n",
        "val_gen = GenerateDataset('/gdrive/My Drive/LibiumNet/Lipread_mp4/', 'val')\n",
        "#val_gen = val_gen.generator()\n",
        "#val_gen = GenerateDataset('/gdrive/My Drive/LibiumNet/overfit_test/', 'test')\n",
        "val_gen = val_gen.generator()\n",
        "\n",
        "model.train(datasets, steps_per_epoch = steps_per_epoch, epochs=10,validation_data=val_gen, validation_steps=steps_per_epoch)\n",
        "model.model.save('/gdrive/My Drive/LibiumNet/libium-0-6.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kjfslS1lpx6u",
        "colab_type": "code",
        "outputId": "14ba0b45-a523-4c5b-9d16-e741374b9740",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 550
        }
      },
      "cell_type": "code",
      "source": [
        "#model = LibiumNet(output_size=2)\n",
        "gen = GenerateDataset('/gdrive/My Drive/LibiumNet/Lipread_mp4/', 'train')\n",
        "datasets = gen.generator()\n",
        "print(model.model.predict(next(datasets)))\n",
        "print(model.model.predict(next(datasets)))\n",
        "print(model.model.predict(next(datasets)))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'directory': ['AFFAIRS', 'HOSPITAL', 'CHINA', 'LEADER', 'EDUCATION'], 'class': [0, 1, 2, 3, 4]}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-37-9d4ce403699f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mgen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGenerateDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/gdrive/My Drive/LibiumNet/Lipread_mp4/'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdatasets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-8c6e0942b595>\u001b[0m in \u001b[0;36m_generator\u001b[0;34m(self, data, directory, video_files, BATCH_SIZE)\u001b[0m\n\u001b[1;32m    151\u001b[0m                     \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'directory'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'_'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m                     \u001b[0;31m#print(data.loc[ data['directory'] == train[i].split('/')[-1].split('_')[-2] ].values[0][1])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m                 \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_categorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNUM_CLASSES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m                 \u001b[0;32myield\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/utils/np_utils.py\u001b[0m in \u001b[0;36mto_categorical\u001b[0;34m(y, num_classes, dtype)\u001b[0m\n\u001b[1;32m     47\u001b[0m   \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m   \u001b[0mcategorical\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m   \u001b[0mcategorical\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m   \u001b[0moutput_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_shape\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0mcategorical\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcategorical\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: index 4 is out of bounds for axis 1 with size 2"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "c5JKapHd7xIA",
        "colab_type": "code",
        "outputId": "ad1938de-dfa7-4dfe-bfab-35e0cb2316d1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 56
        }
      },
      "cell_type": "code",
      "source": [
        "model.model.save('/gdrive/My Drive/LibiumNet/libium-0.2.h5')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file.You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "jDwHTngB5sSL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        " # print(list(os.listdir('/gdrive/My Drive/LibiumNet/lipread_mp4/')))\n",
        "# print(next(datasets))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tlnBkkeEADbm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# def create_df(file_path):\n",
        "#   '''\n",
        "#   creates pandas dataframe of labels and words directories\n",
        "#   '''\n",
        "  \n",
        "#   d = {}\n",
        "#   y_labels = []\n",
        "#   class_folders = []\n",
        "#   for ind, clss in enumerate(os.listdir(file_path)):\n",
        "#     y_labels.append(ind)\n",
        "#     class_folders.append(clss)\n",
        "  \n",
        "#   d['directory'] = class_folders\n",
        "#   d['class'] = y_labels\n",
        "  \n",
        "#   return pd.DataFrame(d)\n",
        "\n",
        "\n",
        "# def get_video_files(file_path, directory=None):\n",
        "#   '''\n",
        "#   get video files from word class directories\n",
        "#   '''\n",
        "#   d = {}\n",
        "#   f = []\n",
        "  \n",
        "#   for root, dirs, files in os.walk(file_path):\n",
        "#     if root.split('/')[-1] == directory:\n",
        "#       for file in files:\n",
        "#         if file.endswith(\".mp4\"):\n",
        "#           target_file = file.split('_')[0]\n",
        "          \n",
        "#           f.append(target_file)\n",
        "            \n",
        "            \n",
        "#           if target_file not in d:\n",
        "#             d[target_file] = []\n",
        "#           d[target_file].append(os.path.join(root, file))\n",
        "          \n",
        "#   return d\n",
        "      \n",
        "  \n",
        "  \n",
        "  \n",
        "# def load_video(filename):\n",
        "#     \"\"\"\n",
        "#     Loads the specified video using ffmpeg.\n",
        "\n",
        "#     Args:\n",
        "#         filename (str): The path to the file to load.\n",
        "#             Should be a format that ffmpeg can handle.\n",
        "\n",
        "#     Returns:\n",
        "#         List[FloatTensor]: the frames of the video as a list of 3D tensors\n",
        "#             (channels, width, height)\n",
        "#     \"\"\"\n",
        "\n",
        "#     reader = imageio.get_reader(filename,  'ffmpeg')\n",
        "        \n",
        "#     return np.array(list(reader), dtype=np.float32)\n",
        "\n",
        "  \n",
        "  \n",
        "  \n",
        "# def generator(data, directory=None, video_files=None, BATCH_SIZE=64):\n",
        "  \n",
        "#   '''\n",
        "#   retrieves the training batch for each iteration\n",
        "#   '''\n",
        "#   IMAGE_HEIGHT = 256\n",
        "#   IMAGE_WIDTH = 256\n",
        "#   IMAGE_CHANNEL = 3\n",
        "#   NUM_FRAMES = 29\n",
        "#   NUM_CLASSES = 10\n",
        "  \n",
        "  \n",
        "#   train = []\n",
        "#   for key, value in video_files.items():\n",
        "#     for file in value:\n",
        "#       train.append(file)\n",
        "  \n",
        "#   while True:\n",
        "#     # Randomize the indices to make an array\n",
        "#     indices_arr = np.random.permutation(len(train))\n",
        "    \n",
        "#     for batch in range(0, len(indices_arr), BATCH_SIZE):\n",
        "#       # slice out the current batch according to batch-size\n",
        "#       current_batch = indices_arr[batch:(batch + BATCH_SIZE)]\n",
        "\n",
        "#       # initializing the arrays, x_train and y_train\n",
        "#       x_train = np.empty([0, NUM_FRAMES, IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_CHANNEL], dtype=np.float32)\n",
        "      \n",
        "#       y_train = np.empty([0], dtype=np.int32)\n",
        "#       for i in current_batch:\n",
        "#           # get an image and its corresponding color for an traffic light\n",
        "#           video_frames = load_video(train[i])\n",
        "\n",
        "# #           Appending them to existing batch\n",
        "#           x_train = np.append(x_train, [video_frames], axis=0)\n",
        "#           y_train = np.append(y_train, [ data.loc[ data['directory'] == train[i].split('/')[-1].split('_')[-2] ].values[0][0] ])\n",
        "          \n",
        "#       y_train = to_categorical(y_train, num_classes=NUM_CLASSES)\n",
        "\n",
        "#       yield(x_train, y_train)\n",
        "    \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HVBabzVbrOu8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# video_files = get_video_files('/gdrive/My Drive/LibiumNet/lipread_mp4/', directory='train')\n",
        "# data = create_df('/gdrive/My Drive/LibiumNet/lipread_mp4/')\n",
        "    \n",
        "# gen_obj = generator(data=data, directory='train', video_files=video_files)\n",
        "# t_x, t_y = next(gen_obj)\n",
        "\n",
        "# print(t_x)\n",
        "# print(t_y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FqTknnIbsY4f",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# libium = LibiumNet(output_size=20)\n",
        "# libium.predict(tensor)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "efuehJPPj7-U",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# class GenerateDataset(object):\n",
        "  \n",
        "#     def __init__(self, dataset_path):\n",
        "      \n",
        "#         self.videos = os.listdir(dataset_path)\n",
        "#         self.all_frames = []\n",
        "        \n",
        "#         for vid in self.videos:\n",
        "#             frames = load_video(vid)\n",
        "#             self.all_frames += frames\n",
        "\n",
        "#     def gen(self):\n",
        "#         for frame in self.all_frames:\n",
        "#             yield(frame)\n",
        "\n",
        "#     def get_dataset(self):\n",
        "#         ds = tf.data.Dataset.from_generator(\n",
        "#             self.gen, (tf.float32)\n",
        "#         )\n",
        "        \n",
        "#         return ds\n",
        "   \n",
        "#     def get_input(path):\n",
        "#       img = imread(path)\n",
        "#       return(img)\n",
        "    \n",
        "    \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AuC3SotRuGR9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class LibiumNet2(object):\n",
        "    \"\"\"TA lipreading model, `LibiunNet`\n",
        "    This is lip reading model which reads or predicts the words of a spoken mouth in a silent video. \n",
        "    This model implements the RCNN (Recurrent Convulutional Neural Network) architecture. \n",
        "\n",
        "    :param img_c: The number of channels of the input image. i.e. a frame in a video (default 3).\n",
        "    :param img_w: The width of the input image i.e. a frame in a video (default 256)\n",
        "    :param img_h: The height of the input image i.e. a frame in a video (default 256)\n",
        "    :param frames_n: The total number of frames in an input video (default 29)\n",
        "    :param output_size: The output size of the network. \n",
        "    \n",
        "    \"\"\"\n",
        "    def __init__(self, img_c=3, img_w=256, img_h=256, frames_n=29, output_size=10):\n",
        "        self.img_c = img_c\n",
        "        self.img_w = img_w\n",
        "        self.img_h = img_h\n",
        "        self.frames_n = frames_n\n",
        "        self.output_size = output_size\n",
        "        self.build()\n",
        "    \n",
        "    def build(self):\n",
        "        \"\"\"\n",
        "        Retrieves the features from the last pool layer in the densenet pretrained model \n",
        "        and pass obtained features to LSTM network. \n",
        "        \"\"\"\n",
        "        input_shape = (self.frames_n, self.img_w, self.img_h, self.img_c) # input shape\n",
        "\n",
        "\n",
        "        ## Getting the pre-trained image classifier, weights='imagenet'\n",
        "        \n",
        "        vgg = tf.keras.applications.nasnet.NASNetMobile(include_top=False, pooling='avg')\n",
        "        \n",
        "        #densenet = tf.keras.applications.densenet.DenseNet201(include_top=True, weights='imagenet')\n",
        "        #for layer in densenet.layers[:-50]:\n",
        "        #    layer.trainable = False\n",
        "\n",
        "\n",
        "        ######################\n",
        "        ## BUILDING THE MODEL\n",
        "        ######################\n",
        "        self.input_data = Input(name='the_input', shape=input_shape, dtype='float32')\n",
        "        self.image_frame_features = TimeDistributed(vgg)(self.input_data) ## extracting the features from the images\n",
        "\n",
        "        #self.drop1 = Dropout(0.3)(self.image_frame_features)\n",
        "        #self.flat = TimeDistributed(Flatten())(self.image_frame_features) ## flatten before passing on to the recurrent network\n",
        "        self.flat = Flatten()(self.image_frame_features)\n",
        "        \n",
        "        #self.norm1 = BatchNormalization()(self.flat)\n",
        "        \n",
        "        #self.sequence1 = CuDNNLSTM(16, return_sequences = True)(self.flat) \n",
        "        #self.sequence = CuDNNLSTM(16)(self.flat) \n",
        "        \n",
        "        #self.norm2 = BatchNormalization()(self.sequence)\n",
        "\n",
        "        #self.drop2 = Dropout(0.1)(self.flat)\n",
        "        \n",
        "        #self.dense7 = Dense(4096, activation='relu')(self.sequence)\n",
        "        #self.dense6 = Dense(1024, activation='relu')(self.dense7)\n",
        "        #self.dense5 = Dense(256, activation='relu')(self.dense6)\n",
        "        #self.dense4 = Dense(64, activation='relu')(self.dense5)\n",
        "        self.dense3 = Dense(16, activation='relu')(self.flat)\n",
        "        self.dense2 = Dense(4, activation='relu')(self.dense3)\n",
        "        \n",
        "#         \n",
        "        #self.dense = Dense(self.output_size, name='dense1')(self.sequence)\n",
        "        self.dense = Dense(self.output_size, name='logits')(self.dense2)\n",
        "\n",
        "        self.pred = Activation('softmax', name='softmax')(self.dense)\n",
        "\n",
        "\n",
        "        self.model = Model(inputs = self.input_data, outputs=self.pred)\n",
        "\n",
        "    def summary(self):\n",
        "        \"\"\"\"Summarizes the architecture of the model.\n",
        "        \n",
        "        :return: returns the model architecture summary\n",
        "        \"\"\"\n",
        "        return self.model.summary()\n",
        "      \n",
        "    \n",
        "    def train(self, generator,steps_per_epoch=None, epochs=1,validation_data=None, validation_steps=None, filepath=\"/gdrive/My Drive/LibiumNet/checkpoint.h5\"):\n",
        "        # Callbacks\n",
        "        early_stopping_monitor = EarlyStopping(patience=3)\n",
        "        checkpoint = ModelCheckpoint(\n",
        "                            filepath, monitor='val_acc', verbose=1, \n",
        "                            save_best_only=True, mode='max'\n",
        "                    )\n",
        "        \n",
        "        reduce_lr = ReduceLROnPlateau(\n",
        "                            monitor='val_loss', factor=0.2,\n",
        "                              patience=7, min_lr=1e-80\n",
        "                    )\n",
        "        \n",
        "        callbacks_list = [checkpoint, reduce_lr, early_stopping_monitor]\n",
        "\n",
        "        \n",
        "        print('Training...')\n",
        "        \n",
        "        self.model.compile(\n",
        "              optimizer=tf.keras.optimizers.RMSprop(1e-7),\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy']\n",
        "        )\n",
        "        \n",
        "        history = self.model.fit_generator(generator, steps_per_epoch=steps_per_epoch, epochs=epochs, validation_data=validation_data, validation_steps = validation_steps)\n",
        "        \n",
        "        #self.visualize_accuracy(history)\n",
        "        self.visualize_loss(history)\n",
        "      \n",
        "      \n",
        "    def predict(self, input_batch):\n",
        "        \"\"\"Predicts a video\n",
        "        \n",
        "        :param input_batch: A batch of a sequence of frames. \n",
        "        :return: returns the predicted probailities\n",
        "        \"\"\"\n",
        "        return self.model(input_batch)\n",
        "      \n",
        "    def visualize_accuracy(self, history):\n",
        "      \"\"\"Visualize model accuracy\n",
        "      \"\"\"\n",
        "      plt.plot(history.history['acc'], label='training accuracy')\n",
        "      plt.plot(history.history['val_acc'], label='testing accuracy')\n",
        "      plt.title('Accuracy')\n",
        "      plt.xlabel('epochs')\n",
        "      plt.ylabel('accuracy')\n",
        "      plt.legend()\n",
        "      \n",
        "    def visualize_loss(self, history):\n",
        "      \"\"\"Visualizes model loss\"\"\"\n",
        "      plt.plot(history.history['loss'], label='training loss')\n",
        "      plt.plot(history.history['val_loss'], label='testing loss')\n",
        "      plt.title('Loss')\n",
        "      plt.xlabel('epochs')\n",
        "      plt.ylabel('loss')\n",
        "      plt.legend()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}